{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\pytorch_quantization\\tensor_quant.py:337: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax < 0:\n",
      "c:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\pytorch_quantization\\tensor_quant.py:340: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
      "c:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\pytorch_quantization\\tensor_quant.py:350: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_amax <= epsilon:  # Treat amax smaller than minimum representable of fp16 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NYI: Named tensors are not supported with the tracer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\TheoA\\Documents\\MIT\\JUNJAUNT\\6.205 FPGA\\Bespoke\\Bespoke\\makesv.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TheoA/Documents/MIT/JUNJAUNT/6.205%20FPGA/Bespoke/Bespoke/makesv.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TheoA/Documents/MIT/JUNJAUNT/6.205%20FPGA/Bespoke/Bespoke/makesv.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model \u001b[39m=\u001b[39m TestModel()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TheoA/Documents/MIT/JUNJAUNT/6.205%20FPGA/Bespoke/Bespoke/makesv.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(model, torch\u001b[39m.\u001b[39;49mzeros(\u001b[39m1\u001b[39;49m, \u001b[39m8\u001b[39;49m), \u001b[39m\"\u001b[39;49m\u001b[39msinglelayer.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, opset_version\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\onnx\\utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[0;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[0;32m    191\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \n\u001b[0;32m    212\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m     _export(\n\u001b[0;32m    517\u001b[0m         model,\n\u001b[0;32m    518\u001b[0m         args,\n\u001b[0;32m    519\u001b[0m         f,\n\u001b[0;32m    520\u001b[0m         export_params,\n\u001b[0;32m    521\u001b[0m         verbose,\n\u001b[0;32m    522\u001b[0m         training,\n\u001b[0;32m    523\u001b[0m         input_names,\n\u001b[0;32m    524\u001b[0m         output_names,\n\u001b[0;32m    525\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[0;32m    526\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[0;32m    527\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[0;32m    528\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[0;32m    529\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[0;32m    530\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[0;32m    531\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[0;32m    532\u001b[0m         autograd_inlining\u001b[39m=\u001b[39;49mautograd_inlining,\n\u001b[0;32m    533\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\onnx\\utils.py:1596\u001b[0m, in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[0;32m   1593\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1594\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[1;32m-> 1596\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[0;32m   1597\u001b[0m     model,\n\u001b[0;32m   1598\u001b[0m     args,\n\u001b[0;32m   1599\u001b[0m     verbose,\n\u001b[0;32m   1600\u001b[0m     input_names,\n\u001b[0;32m   1601\u001b[0m     output_names,\n\u001b[0;32m   1602\u001b[0m     operator_export_type,\n\u001b[0;32m   1603\u001b[0m     val_do_constant_folding,\n\u001b[0;32m   1604\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[0;32m   1605\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[0;32m   1606\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[0;32m   1607\u001b[0m )\n\u001b[0;32m   1609\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[0;32m   1610\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[0;32m   1611\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[0;32m   1612\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\onnx\\utils.py:1135\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[1;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[0;32m   1132\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m   1134\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[1;32m-> 1135\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[0;32m   1136\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[0;32m   1138\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\onnx\\utils.py:1011\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m   1006\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[0;32m   1007\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m     )\n\u001b[0;32m   1009\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1011\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[0;32m   1012\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m   1013\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\onnx\\utils.py:915\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m    913\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[0;32m    914\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 915\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[0;32m    916\u001b[0m     model,\n\u001b[0;32m    917\u001b[0m     args,\n\u001b[0;32m    918\u001b[0m     strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    919\u001b[0m     _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    920\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    921\u001b[0m )\n\u001b[0;32m    922\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[0;32m    924\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\jit\\_trace.py:1285\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[1;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m   1284\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m-> 1285\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(\n\u001b[0;32m   1286\u001b[0m     f, strict, _force_outplace, return_inputs, _return_inputs_states\n\u001b[0;32m   1287\u001b[0m )(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1288\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\jit\\_trace.py:133\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[1;32m--> 133\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[0;32m    134\u001b[0m     wrapper,\n\u001b[0;32m    135\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[0;32m    136\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[0;32m    137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[0;32m    138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[0;32m    139\u001b[0m )\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\jit\\_trace.py:124\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    123\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[1;32m--> 124\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[0;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    126\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\nn\\modules\\module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "\u001b[1;32mc:\\Users\\TheoA\\Documents\\MIT\\JUNJAUNT\\6.205 FPGA\\Bespoke\\Bespoke\\makesv.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/TheoA/Documents/MIT/JUNJAUNT/6.205%20FPGA/Bespoke/Bespoke/makesv.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/TheoA/Documents/MIT/JUNJAUNT/6.205%20FPGA/Bespoke/Bespoke/makesv.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(x))\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\nn\\modules\\module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\pytorch_quantization\\nn\\modules\\quant_linear.py:70\u001b[0m, in \u001b[0;36mQuantLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m---> 70\u001b[0m     quant_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_quantizer(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m     71\u001b[0m     quant_weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_weight_quantizer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n\u001b[0;32m     73\u001b[0m     output \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlinear(quant_input, quant_weight, bias\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\nn\\modules\\module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1506\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1508\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1509\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1510\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\pytorch_quantization\\nn\\modules\\tensor_quantizer.py:346\u001b[0m, in \u001b[0;36mTensorQuantizer.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    343\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip(inputs)\n\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_if_quant:\n\u001b[1;32m--> 346\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_quant_forward(inputs)\n\u001b[0;32m    348\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\pytorch_quantization\\nn\\modules\\tensor_quantizer.py:310\u001b[0m, in \u001b[0;36mTensorQuantizer._quant_forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fake_quant:\n\u001b[0;32m    309\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m TensorQuantizer\u001b[39m.\u001b[39muse_fb_fake_quant:\n\u001b[1;32m--> 310\u001b[0m         outputs \u001b[39m=\u001b[39m fake_tensor_quant(inputs, amax, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_bits, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_unsigned, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_narrow_range)\n\u001b[0;32m    311\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m         \u001b[39mif\u001b[39;00m inputs\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mhalf \u001b[39mor\u001b[39;00m amax\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mhalf:\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\torch\\autograd\\function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    537\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    538\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mapply(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[0;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\pytorch_quantization\\tensor_quant.py:306\u001b[0m, in \u001b[0;36mFakeTensorQuantFunction.forward\u001b[1;34m(ctx, inputs, amax, num_bits, unsigned, narrow_range)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    304\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx, inputs, amax, num_bits\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, unsigned\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, narrow_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    305\u001b[0m     ctx\u001b[39m.\u001b[39msave_for_backward(inputs, amax)\n\u001b[1;32m--> 306\u001b[0m     outputs, scale \u001b[39m=\u001b[39m _tensor_quant(inputs, amax, num_bits, unsigned, narrow_range)\n\u001b[0;32m    307\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs \u001b[39m/\u001b[39m scale\u001b[39m.\u001b[39mto(inputs\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\TheoA\\Anaconda3\\envs\\bespoke\\lib\\site-packages\\pytorch_quantization\\tensor_quant.py:352\u001b[0m, in \u001b[0;36m_tensor_quant\u001b[1;34m(inputs, amax, num_bits, unsigned, narrow_range)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[39mif\u001b[39;00m min_amax \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m epsilon:  \u001b[39m# Treat amax smaller than minimum representable of fp16 0\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     zero_amax_mask \u001b[39m=\u001b[39m (amax \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m epsilon)\n\u001b[1;32m--> 352\u001b[0m     scale[zero_amax_mask] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m  \u001b[39m# Value quantized with amax=0 should all be 0\u001b[39;00m\n\u001b[0;32m    354\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp((inputs \u001b[39m*\u001b[39m scale)\u001b[39m.\u001b[39mround_(), min_bound, max_bound)\n\u001b[0;32m    356\u001b[0m \u001b[39mif\u001b[39;00m min_amax \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m epsilon:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: NYI: Named tensors are not supported with the tracer"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import pytorch_quantization\n",
    "from pytorch_quantization import tensor_quant\n",
    "import pytorch_quantization.nn as quant_nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "\n",
    "\n",
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = quant_nn.Linear(8, 4, bias=True,\n",
    "                                            quant_desc_input=tensor_quant.QUANT_DESC_8BIT_PER_TENSOR,\n",
    "                                            quant_desc_weight=tensor_quant.QUANT_DESC_8BIT_LINEAR_WEIGHT_PER_ROW)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.linear(x))\n",
    "model = TestModel()\n",
    "torch.onnx.export(model, torch.zeros(1, 8), \"singlelayer.onnx\", verbose=True, opset_version=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic\n",
    "quantize_dynamic(\n",
    "        \"singlelayer.onnx\",\n",
    "        \"singlelayer.quantized.onnx\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph tf2onnx (\n",
      "  %serving_default_dense_input:0[INT8, unk__44x1]\n",
      ") initializers (\n",
      "  %zero_point__41[INT8, scalar]\n",
      "  %zero_point__39[INT8, scalar]\n",
      "  %zero_point__29[INT8, scalar]\n",
      "  %zero_point__23[INT32, scalar]\n",
      "  %sequential/dense_2/MatMul[INT8, 16x1]\n",
      "  %sequential/dense_2/BiasAdd/ReadVariableOp[INT32, 1]\n",
      "  %sequential/dense_1/MatMul[INT8, 16x16]\n",
      "  %sequential/dense_1/BiasAdd/ReadVariableOp[INT32, 16]\n",
      "  %sequential/dense/MatMul[INT8, 1x16]\n",
      "  %sequential/dense/BiasAdd/ReadVariableOp[INT32, 16]\n",
      "  %scale__40[FLOAT, scalar]\n",
      "  %scale__38[FLOAT, scalar]\n",
      "  %scale__34[FLOAT, scalar]\n",
      "  %scale__30[FLOAT, scalar]\n",
      "  %scale__28[FLOAT, scalar]\n",
      "  %scale__26[FLOAT, scalar]\n",
      "  %scale__24[FLOAT, scalar]\n",
      "  %scale__22[FLOAT, scalar]\n",
      "  %scale__20[FLOAT, scalar]\n",
      "  %scale__18[FLOAT, scalar]\n",
      ") {\n",
      "  %sequential/dense/BiasAdd/ReadVariableOp_dequant = DequantizeLinear(%sequential/dense/BiasAdd/ReadVariableOp, %scale__30, %zero_point__23)\n",
      "  %sequential/dense/MatMul_dequant = DequantizeLinear(%sequential/dense/MatMul, %scale__28, %zero_point__29)\n",
      "  %sequential/dense_1/BiasAdd/ReadVariableOp_dequant = DequantizeLinear(%sequential/dense_1/BiasAdd/ReadVariableOp, %scale__26, %zero_point__23)\n",
      "  %sequential/dense_1/MatMul_dequant = DequantizeLinear(%sequential/dense_1/MatMul, %scale__24, %zero_point__29)\n",
      "  %sequential/dense_2/BiasAdd/ReadVariableOp_dequant = DequantizeLinear(%sequential/dense_2/BiasAdd/ReadVariableOp, %scale__22, %zero_point__23)\n",
      "  %sequential/dense_2/MatMul_dequant = DequantizeLinear(%sequential/dense_2/MatMul, %scale__20, %zero_point__29)\n",
      "  %serving_default_dense_input:0_dequant = DequantizeLinear(%serving_default_dense_input:0, %scale__18, %zero_point__39)\n",
      "  %sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd_prequant = MatMul(%serving_default_dense_input:0_dequant, %sequential/dense/MatMul_dequant)\n",
      "  %Add__8:0 = Add(%sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd_prequant, %sequential/dense/BiasAdd/ReadVariableOp_dequant)\n",
      "  %Relu__5:0 = Relu(%Add__8:0)\n",
      "  %sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd = QuantizeLinear(%Relu__5:0, %scale__34, %zero_point__39)\n",
      "  %sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd_dequant = DequantizeLinear(%sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd, %scale__34, %zero_point__39)\n",
      "  %sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd_prequant = MatMul(%sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd_dequant, %sequential/dense_1/MatMul_dequant)\n",
      "  %Add__13:0 = Add(%sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd_prequant, %sequential/dense_1/BiasAdd/ReadVariableOp_dequant)\n",
      "  %Relu__10:0 = Relu(%Add__13:0)\n",
      "  %sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd = QuantizeLinear(%Relu__10:0, %scale__38, %zero_point__39)\n",
      "  %sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd_dequant = DequantizeLinear(%sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd, %scale__38, %zero_point__39)\n",
      "  %StatefulPartitionedCall:0_prequant = MatMul(%sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd_dequant, %sequential/dense_2/MatMul_dequant)\n",
      "  %Add__16:0 = Add(%StatefulPartitionedCall:0_prequant, %sequential/dense_2/BiasAdd/ReadVariableOp_dequant)\n",
      "  %StatefulPartitionedCall:0 = QuantizeLinear(%Add__16:0, %scale__40, %zero_point__41)\n",
      "  return %StatefulPartitionedCall:0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"hello_world_int8.onnx\")\n",
    "onnx_model = onnx.shape_inference.infer_shapes(onnx_model)\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(onnx_model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_type: 3\n",
      "name: \"zero_point__41\"\n",
      "raw_data: \"\\005\"\n",
      "\n",
      "data_type: 3\n",
      "name: \"zero_point__39\"\n",
      "raw_data: \"\\200\"\n",
      "\n",
      "data_type: 3\n",
      "name: \"zero_point__29\"\n",
      "raw_data: \"\\000\"\n",
      "\n",
      "data_type: 6\n",
      "name: \"zero_point__23\"\n",
      "raw_data: \"\\000\\000\\000\\000\"\n",
      "\n",
      "dims: 16\n",
      "dims: 1\n",
      "data_type: 3\n",
      "name: \"sequential/dense_2/MatMul\"\n",
      "raw_data: \"\\331;\\'\\025\\034\\340\\336\\335\\017\\033\\305\\327\\022\\335\\371\\177\"\n",
      "\n",
      "dims: 1\n",
      "data_type: 6\n",
      "name: \"sequential/dense_2/BiasAdd/ReadVariableOp\"\n",
      "raw_data: \"\\255\\001\\000\\000\"\n",
      "\n",
      "dims: 16\n",
      "dims: 16\n",
      "data_type: 3\n",
      "name: \"sequential/dense_1/MatMul\"\n",
      "raw_data: \"\\364\\003\\373\\020\\344#\\021\\335%\\357\\352\\342\\333\\031\\351\\010\\032\\334\\'\\376\\025\\n\\014\\373!\\034\\032\\037\\347\\036$\\\"\\355\\322\\335\\024\\007\\\"\\365\\375\\035\\347\\374\\351\\025\\342\\346&\\t\\002\\353\\332\\013$\\340\\000\\n\\003\\330\\327\\025\\t)!\\031\\006\\333\\034\\004\\036\\363\\024\\333\\340\\023\\312\\365\\375\\000\\t!\\371\\344\\370\\033\\'\\007&\\035\\026\\000\\342\\006\\363\\007\\010\\364\\364\\005\\363\\032\\003\\027\\021\\334\\002\\304\\335\\326\\024\\026\\017$\\002\\032\\361\\376\\346\\345\\027 \\003\\330\\306\\032\\335\\342\\013\\340\\377\\027\\357\\353\\003\\350\\347\\001!\\354\\023\\334\\332\\036\\340!\\372\\374\\342\\001$\\355\\361\\372\\030\\331\\347\\t \\r\\022\\357\\357$\\363\\336\\377\\372\\021\\343\\t\\376\\004\\\"\\331#\\364\\274\\361\\022\\t!\\300\\334\\3527.\\r>\\376\\017\\323\\177\\367\\357\\025\\343\\346\\021\\350\\002\\013\\331\\031\\000\\010\\343\\335\\334\\365\\323\\357\\351\\013\\370#&\\361\\345 \\001\\002\\371\\367X\\372\\'\\036\\355\\354\\374\\373\\004\\032\\024\\330\\024\\023\\367\\024\\345\\031\\341\\344\\343\\003\\361\\007\\004\\026\\013\\326\\307\\357\\356\\372&\"\n",
      "\n",
      "dims: 16\n",
      "data_type: 6\n",
      "name: \"sequential/dense_1/BiasAdd/ReadVariableOp\"\n",
      "raw_data: \"\\'\\375\\377\\377\\242\\007\\000\\000b\\002\\000\\000\\000\\000\\000\\000\\361\\000\\000\\000)\\376\\377\\377\\335\\377\\377\\377\\235\\374\\377\\377;\\002\\000\\000E\\002\\000\\000\\244\\020\\000\\000g\\017\\000\\000O\\002\\000\\000\\000\\000\\000\\000\\207\\374\\377\\377\\021\\354\\377\\377\"\n",
      "\n",
      "dims: 1\n",
      "dims: 16\n",
      "data_type: 3\n",
      "name: \"sequential/dense/MatMul\"\n",
      "raw_data: \"\\367\\3129Ghsbc@\\346\\177\\031\\256D_V\"\n",
      "\n",
      "dims: 16\n",
      "data_type: 6\n",
      "name: \"sequential/dense/BiasAdd/ReadVariableOp\"\n",
      "raw_data: \"\\000\\000\\000\\000\\000\\000\\000\\000\\302\\352\\377\\377u\\352\\377\\377\\270\\372\\377\\377$\\372\\377\\377\\310\\357\\377\\377\\254\\377\\377\\377D\\r\\000\\000\\000\\000\\000\\000\\275\\007\\000\\0003\\352\\377\\377\\000\\000\\000\\000\\314\\344\\377\\377O\\r\\000\\000\\317\\343\\377\\377\"\n",
      "\n",
      "data_type: 1\n",
      "name: \"scale__40\"\n",
      "raw_data: \"\\313\\326\\007<\"\n",
      "\n",
      "data_type: 1\n",
      "name: \"scale__38\"\n",
      "raw_data: \"]OQ<\"\n",
      "\n",
      "data_type: 1\n",
      "name: \"scale__34\"\n",
      "raw_data: \"\\237QZ<\"\n",
      "\n",
      "data_type: 1\n",
      "name: \"scale__30\"\n",
      "raw_data: \"U[\\3178\"\n",
      "\n",
      "data_type: 1\n",
      "name: \"scale__28\"\n",
      "raw_data: \"\\252Y\\204;\"\n",
      "\n",
      "data_type: 1\n",
      "name: \"scale__26\"\n",
      "raw_data: \"{9\\0309\"\n",
      "\n",
      "data_type: 1\n",
      "name: \"scale__24\"\n",
      "raw_data: \"\\177\\1772<\"\n",
      "\n",
      "data_type: 1\n",
      "name: \"scale__22\"\n",
      "raw_data: \"\\313AN9\"\n",
      "\n",
      "data_type: 1\n",
      "name: \"scale__20\"\n",
      "raw_data: \"\\027D|<\"\n",
      "\n",
      "data_type: 1\n",
      "name: \"scale__18\"\n",
      "raw_data: \"\\206\\212\\310<\"\n",
      "\n",
      "input: \"sequential/dense/BiasAdd/ReadVariableOp\"\n",
      "input: \"scale__30\"\n",
      "input: \"zero_point__23\"\n",
      "output: \"sequential/dense/BiasAdd/ReadVariableOp_dequant\"\n",
      "name: \"sequential/dense/BiasAdd/ReadVariableOp_dequant\"\n",
      "op_type: \"DequantizeLinear\"\n",
      "\n",
      "input: \"sequential/dense/MatMul\"\n",
      "input: \"scale__28\"\n",
      "input: \"zero_point__29\"\n",
      "output: \"sequential/dense/MatMul_dequant\"\n",
      "name: \"sequential/dense/MatMul_dequant\"\n",
      "op_type: \"DequantizeLinear\"\n",
      "\n",
      "input: \"sequential/dense_1/BiasAdd/ReadVariableOp\"\n",
      "input: \"scale__26\"\n",
      "input: \"zero_point__23\"\n",
      "output: \"sequential/dense_1/BiasAdd/ReadVariableOp_dequant\"\n",
      "name: \"sequential/dense_1/BiasAdd/ReadVariableOp_dequant\"\n",
      "op_type: \"DequantizeLinear\"\n",
      "\n",
      "input: \"sequential/dense_1/MatMul\"\n",
      "input: \"scale__24\"\n",
      "input: \"zero_point__29\"\n",
      "output: \"sequential/dense_1/MatMul_dequant\"\n",
      "name: \"sequential/dense_1/MatMul_dequant\"\n",
      "op_type: \"DequantizeLinear\"\n",
      "\n",
      "input: \"sequential/dense_2/BiasAdd/ReadVariableOp\"\n",
      "input: \"scale__22\"\n",
      "input: \"zero_point__23\"\n",
      "output: \"sequential/dense_2/BiasAdd/ReadVariableOp_dequant\"\n",
      "name: \"sequential/dense_2/BiasAdd/ReadVariableOp_dequant\"\n",
      "op_type: \"DequantizeLinear\"\n",
      "\n",
      "input: \"sequential/dense_2/MatMul\"\n",
      "input: \"scale__20\"\n",
      "input: \"zero_point__29\"\n",
      "output: \"sequential/dense_2/MatMul_dequant\"\n",
      "name: \"sequential/dense_2/MatMul_dequant\"\n",
      "op_type: \"DequantizeLinear\"\n",
      "\n",
      "input: \"serving_default_dense_input:0\"\n",
      "input: \"scale__18\"\n",
      "input: \"zero_point__39\"\n",
      "output: \"serving_default_dense_input:0_dequant\"\n",
      "name: \"serving_default_dense_input:0_dequant\"\n",
      "op_type: \"DequantizeLinear\"\n",
      "\n",
      "input: \"serving_default_dense_input:0_dequant\"\n",
      "input: \"sequential/dense/MatMul_dequant\"\n",
      "output: \"sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd_prequant\"\n",
      "name: \"sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd_prequant\"\n",
      "op_type: \"MatMul\"\n",
      "\n",
      "input: \"sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd_prequant\"\n",
      "input: \"sequential/dense/BiasAdd/ReadVariableOp_dequant\"\n",
      "output: \"Add__8:0\"\n",
      "name: \"Add__8\"\n",
      "op_type: \"Add\"\n",
      "\n",
      "input: \"Add__8:0\"\n",
      "output: \"Relu__5:0\"\n",
      "name: \"Relu__5\"\n",
      "op_type: \"Relu\"\n",
      "\n",
      "input: \"Relu__5:0\"\n",
      "input: \"scale__34\"\n",
      "input: \"zero_point__39\"\n",
      "output: \"sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd\"\n",
      "name: \"sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd_quantize\"\n",
      "op_type: \"QuantizeLinear\"\n",
      "\n",
      "input: \"sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd\"\n",
      "input: \"scale__34\"\n",
      "input: \"zero_point__39\"\n",
      "output: \"sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd_dequant\"\n",
      "name: \"sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd_dequant\"\n",
      "op_type: \"DequantizeLinear\"\n",
      "\n",
      "input: \"sequential/dense/MatMul;sequential/dense/Relu;sequential/dense/BiasAdd_dequant\"\n",
      "input: \"sequential/dense_1/MatMul_dequant\"\n",
      "output: \"sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd_prequant\"\n",
      "name: \"sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd_prequant\"\n",
      "op_type: \"MatMul\"\n",
      "\n",
      "input: \"sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd_prequant\"\n",
      "input: \"sequential/dense_1/BiasAdd/ReadVariableOp_dequant\"\n",
      "output: \"Add__13:0\"\n",
      "name: \"Add__13\"\n",
      "op_type: \"Add\"\n",
      "\n",
      "input: \"Add__13:0\"\n",
      "output: \"Relu__10:0\"\n",
      "name: \"Relu__10\"\n",
      "op_type: \"Relu\"\n",
      "\n",
      "input: \"Relu__10:0\"\n",
      "input: \"scale__38\"\n",
      "input: \"zero_point__39\"\n",
      "output: \"sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd\"\n",
      "name: \"sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd_quantize\"\n",
      "op_type: \"QuantizeLinear\"\n",
      "\n",
      "input: \"sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd\"\n",
      "input: \"scale__38\"\n",
      "input: \"zero_point__39\"\n",
      "output: \"sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd_dequant\"\n",
      "name: \"sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd_dequant\"\n",
      "op_type: \"DequantizeLinear\"\n",
      "\n",
      "input: \"sequential/dense_1/MatMul;sequential/dense_1/Relu;sequential/dense_1/BiasAdd_dequant\"\n",
      "input: \"sequential/dense_2/MatMul_dequant\"\n",
      "output: \"StatefulPartitionedCall:0_prequant\"\n",
      "name: \"StatefulPartitionedCall:0_prequant\"\n",
      "op_type: \"MatMul\"\n",
      "\n",
      "input: \"StatefulPartitionedCall:0_prequant\"\n",
      "input: \"sequential/dense_2/BiasAdd/ReadVariableOp_dequant\"\n",
      "output: \"Add__16:0\"\n",
      "name: \"Add__16\"\n",
      "op_type: \"Add\"\n",
      "\n",
      "input: \"Add__16:0\"\n",
      "input: \"scale__40\"\n",
      "input: \"zero_point__41\"\n",
      "output: \"StatefulPartitionedCall:0\"\n",
      "name: \"StatefulPartitionedCall:0_quantize\"\n",
      "op_type: \"QuantizeLinear\"\n",
      "domain: \"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for init in onnx_model.graph.initializer:\n",
    "    print(init)\n",
    "for node in onnx_model.graph.node:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_node = onnx_model.graph.node[0]\n",
    "init_node.input[1]\n",
    "onnx_model.graph.initializer\n",
    "next(i for i in iter(onnx_model.graph.initializer) if i.name == \"linear.weight\").float_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (parsemodel.py, line 34)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/.conda/envs/bespoke/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3550\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 1\u001b[0;36m\n\u001b[0;31m    from compiler import parsemodel, fpgamodule\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m/mnt/c/Users/TheoA/Documents/MIT/JUNJAUNT/6.205 FPGA/Bespoke/Bespoke/compiler/parsemodel.py:34\u001b[0;36m\u001b[0m\n\u001b[0;31m    if node.op_type == \"MatMul\":\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from compiler import parsemodel, fpgamodule\n",
    "spec = fpgamodule.FPGASpec(120, 1000, 10_000, 100_000)\n",
    "fpga_module = parsemodel.parse_model(onnx_model, 8, 4, spec)\n",
    "pprint(list(mod for mod in fpga_module.modules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpga_module.alloc_regs()\n",
    "fpga_module.alloc_bram()\n",
    "sv = fpga_module.make_sv()\n",
    "with open(\"dummy_model.sv\", \"w\") as f:\n",
    "    f.write(sv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bespoke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
