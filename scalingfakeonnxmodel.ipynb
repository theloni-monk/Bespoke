{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import onnx\n",
    "from onnx import helper\n",
    "from onnx import TensorProto\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is:\n",
      "ir_version: 9\n",
      "graph {\n",
      "  node {\n",
      "    input: \"X_0\"\n",
      "    input: \"weight_0\"\n",
      "    output: \"XMM32_0\"\n",
      "    name: \"mm_0\"\n",
      "    op_type: \"MatMulInteger\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"XMM32_0\"\n",
      "    input: \"bias_0\"\n",
      "    output: \"XMMB32_0\"\n",
      "    name: \"add_0\"\n",
      "    op_type: \"Add\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"XMMB32_0\"\n",
      "    output: \"XMMB8_0\"\n",
      "    op_type: \"Cast\"\n",
      "    attribute {\n",
      "      name: \"to\"\n",
      "      i: 3\n",
      "      type: INT\n",
      "    }\n",
      "  }\n",
      "  node {\n",
      "    input: \"XMMB8_0\"\n",
      "    output: \"Y_0\"\n",
      "    name: \"relu_0\"\n",
      "    op_type: \"Relu\"\n",
      "  }\n",
      "  name: \"testmodel_0\"\n",
      "  initializer {\n",
      "    dims: 8\n",
      "    dims: 8\n",
      "    data_type: 3\n",
      "    name: \"weight_0\"\n",
      "    raw_data: \"\\377\\000\\377\\000\\377\\001\\001\\000\\377\\001\\001\\001\\377\\001\\000\\377\\377\\001\\000\\377\\001\\000\\377\\001\\377\\000\\377\\377\\377\\000\\001\\000\\377\\377\\001\\000\\000\\377\\377\\001\\001\\377\\377\\377\\001\\000\\001\\001\\001\\000\\001\\001\\000\\377\\001\\000\\001\\001\\377\\377\\377\\001\\000\\001\"\n",
      "  }\n",
      "  initializer {\n",
      "    dims: 8\n",
      "    data_type: 6\n",
      "    name: \"bias_0\"\n",
      "    raw_data: \"\\377\\377\\377\\377\\001\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\000\\001\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\000\\377\\377\\377\\377\"\n",
      "  }\n",
      "  input {\n",
      "    name: \"X_0\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 3\n",
      "        shape {\n",
      "          dim {\n",
      "            dim_value: 8\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  output {\n",
      "    name: \"Y_0\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 3\n",
      "        shape {\n",
      "          dim {\n",
      "            dim_value: 8\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "opset_import {\n",
      "  version: 19\n",
      "}\n",
      "\n",
      "The model is checked!\n"
     ]
    }
   ],
   "source": [
    "in_dim = 8\n",
    "out_dim = 8\n",
    "def make_layer(in_dim, out_dim, idx):\n",
    "    # Create one input (ValueInfoPro    to)\n",
    "    X = helper.make_tensor_value_info(f\"X_{idx}\", TensorProto.INT8, [in_dim])\n",
    "    w = helper.make_tensor(f\"weight_{idx}\", TensorProto.INT8, [out_dim, in_dim], np.random.randint(-1, 2, (out_dim, in_dim)).astype(np.int8).tobytes(), raw=True)\n",
    "    b = helper.make_tensor(f\"bias_{idx}\", TensorProto.INT32, [out_dim], np.random.randint(-1, 2, out_dim).astype(np.int32).tobytes(),  raw=True)\n",
    "\n",
    "    # Create one output (ValueInfoProto)\n",
    "    Y = helper.make_tensor_value_info(f\"Y_{idx}\", TensorProto.INT8, [out_dim])\n",
    "\n",
    "    mmnode = helper.make_node(\n",
    "        \"MatMulInteger\",\n",
    "        [f\"X_{idx}\", f\"weight_{idx}\"],\n",
    "        [f\"XMM32_{idx}\"],\n",
    "        name=f\"mm_{idx}\"\n",
    "    )\n",
    "\n",
    "    biasnode = helper.make_node(\n",
    "        \"Add\",\n",
    "        [f\"XMM32_{idx}\", f\"bias_{idx}\"],\n",
    "        [f\"XMMB32_{idx}\"],\n",
    "        name=f\"add_{idx}\"\n",
    "    )\n",
    "    castnode = helper.make_node(\n",
    "        \"Cast\",\n",
    "        [f\"XMMB32_{idx}\"],\n",
    "        [f\"XMMB8_{idx}\"],\n",
    "        to=TensorProto.INT8\n",
    "    )\n",
    "\n",
    "    relunode = helper.make_node(\n",
    "        \"Relu\",\n",
    "        [f\"XMMB8_{idx}\"],\n",
    "        [f\"Y_{idx}\"],\n",
    "        name=f\"relu_{idx}\"\n",
    "    )\n",
    "\n",
    "    # Create the graph (GraphProto)\n",
    "    graph_def = helper.make_graph(\n",
    "        [mmnode,  biasnode, castnode, relunode],\n",
    "        f\"testmodel_{idx}\",\n",
    "        [X],\n",
    "        [Y],\n",
    "        [w, b]\n",
    "    )\n",
    "\n",
    "    # Create the model (ModelProto)\n",
    "    opset = onnx.OperatorSetIdProto()\n",
    "    opset.version = 19\n",
    "    return helper.make_model(graph_def, opset_imports = [opset])\n",
    "\n",
    "model_def = make_layer(8, 8, 0)\n",
    "print(f\"The model is:\\n{model_def}\")\n",
    "onnx.checker.check_model(model_def)\n",
    "onnx.shape_inference.infer_shapes(model_def, check_type=True, strict_mode=True, data_prop=True)\n",
    "print(\"The model is checked!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 12, 40, 60, 126, 168, 288, 360, 550]\n"
     ]
    }
   ],
   "source": [
    "def make_multilayer(n_layers, layer_widths = None):\n",
    "    if layer_widths == None:\n",
    "        layer_widths = [n_layers for _ in range(2*n_layers)]\n",
    "    layers = [make_layer(layer_widths[2*i], layer_widths[2*i+1], i) for i in range(n_layers)]\n",
    "    n_params = sum(layer_widths[2*i]*layer_widths[2*i + 1] + layer_widths[2*i+1] for i in range(n_layers//2))\n",
    "    model = layers[0]\n",
    "    for idx, l in enumerate(layers[1:-1]):\n",
    "        model = onnx.compose.merge_models(model, l, [(f\"Y_{idx}\", f\"X_{idx+1}\")])\n",
    "    if n_layers > 1:\n",
    "        model = onnx.compose.merge_models(model, layers[-1], [(f\"Y_{n_layers-2}\", f\"X_{n_layers-1}\")])\n",
    "    return model, n_params\n",
    "print([make_multilayer(i)[1] for i in range(2, 11)])\n",
    "onnx.checker.check_model(make_multilayer(3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done generating\n"
     ]
    }
   ],
   "source": [
    "from compiler import parsemodel, fpgamodule\n",
    "\n",
    "onnx_model = make_multilayer(1, [8,8,8])[0]\n",
    "#print(onnx_model)\n",
    "spec = fpgamodule.FPGASpec(120, 600_000, 2_700_000, 100_000)\n",
    "fpga_module = parsemodel.parse_model(onnx_model, 8, spec)\n",
    "#pprint(list(mod for mod in fpga_module.modules))\n",
    "fpga_module.alloc_regs()\n",
    "fpga_module.alloc_bram()\n",
    "sv = fpga_module.make_sv()\n",
    "with open(\"dummy_model.sv\", \"w\") as f:\n",
    "    f.write(sv)\n",
    "print(\"done generating\")\n",
    "onnx.save(onnx_model, \"testmodel.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "[array([0, 1, 0, 0, 1, 1, 1, 1], dtype=int8)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "providers = [\"CPUExecutionProvider\"]\n",
    "print(ort.get_available_providers())\n",
    "options = ort.SessionOptions()\n",
    "options.enable_profiling=False\n",
    "sess = ort.InferenceSession(\"testmodel.onnx\", sess_options=options, providers=providers)\n",
    "x_test = np.zeros(8).astype(np.int8)\n",
    "res = sess.run([\"Y_0\"], {\"X_0\": x_test})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.1\n",
      "1.16.3\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "print(onnx.__version__)\n",
    "import onnxruntime\n",
    "print(onnxruntime.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bespoke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
